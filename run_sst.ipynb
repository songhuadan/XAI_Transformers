{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/songhuadan/xai_transformers.git"
      ],
      "metadata": {
        "id": "YDVJp7YZSg1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(f'./XAI_Transformers')"
      ],
      "metadata": {
        "id": "cJngzMXkStEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7IBKu8AQDxU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import inspect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "parentdir = os.path.dirname(currentdir)\n",
        "sys.path.insert(0, parentdir) "
      ],
      "metadata": {
        "id": "y8UBoXQ9QM5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xai_transformer import make_p_layer, BertPooler, BertAttention\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "from sst import get_sst_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, load_metric\n",
        "from attribution import softmax, compute_joint_attention, get_flow_relevance_for_all_layers , _compute_rollout_attention\n",
        "from utils import flip, set_up_dir\n",
        "import torch.multiprocessing\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "1Neod1WmQNq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")"
      ],
      "metadata": {
        "id": "1jBL3pitQRE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "datasets  = load_dataset(\"glue\", 'sst2')\n",
        "\n",
        "\n",
        "label_to_id = {v: i for i, v in enumerate([0,1])}\n",
        "\n",
        "_, test_data_loader = get_sst_dataset(datasets, tokenizer)"
      ],
      "metadata": {
        "id": "B3FH0SMvQToO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init Model\n",
        "class Config(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.hidden_size = 768\n",
        "        self.num_attention_heads = 12\n",
        "        self.layer_norm_eps = 1e-12\n",
        "        self.n_classes = 2\n",
        "        self.n_blocks = 3\n",
        "                    \n",
        "        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        \n",
        "        self.detach_layernorm = True # Detaches the attention-block-output LayerNorm\n",
        "        self.detach_kq = True # Detaches the kq-softmax branch\n",
        "        self.device = device\n",
        "        self.train_mode = False\n",
        "        self.detach_mean = True #\n",
        "\n",
        "        \n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
        "bert_model.bert.embeddings.requires_grad = False\n",
        "for name, param in bert_model.named_parameters():                \n",
        "    if name.startswith('embeddings'):\n",
        "        param.requires_grad = False\n",
        "        \n",
        "pretrained_embeds = bert_model.bert.embeddings\n",
        "\n",
        "params = torch.load('SST/sst2-3layer-model.pt', map_location=torch.device(device))\n",
        "\n",
        "def rename_params(key):\n",
        "    for k_ in ['key','query', 'value']:\n",
        "        key=key.replace(k_, 'p'+k_)\n",
        "    return key"
      ],
      "metadata": {
        "id": "md87Q7qcQYog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model \n",
        "config = Config()\n",
        "config.detach_layernorm = False # Detaches the attention-block-output LayerNorm\n",
        "config.detach_kq = False\n",
        "model = BertAttention(config, pretrained_embeds)\n",
        "model.load_state_dict(params)\n",
        "model.to(device)\n",
        "models = {'none': model}\n",
        "\n",
        "# Transformer Model \n",
        "print('Detach SM')\n",
        "config = Config()\n",
        "config.detach_layernorm = False # Detaches the attention-block-output LayerNorm\n",
        "config.detach_kq = True\n",
        "model = BertAttention(config, pretrained_embeds)\n",
        "model.load_state_dict(params)\n",
        "model.to(device)\n",
        "models['detach_KQ'] = model"
      ],
      "metadata": {
        "id": "C8KbFAYqQdq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Detach SM +  LN')\n",
        "# Transformer Model \n",
        "config = Config()\n",
        "model = BertAttention(config, pretrained_embeds)\n",
        "model.load_state_dict(params)\n",
        "model.to(device)\n",
        "models['detach_KQ_LNorm'] = model"
      ],
      "metadata": {
        "id": "xjtslK_tQjQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Detach SM +  LN without mean')\n",
        "# Transformer Model \n",
        "config = Config()\n",
        "config.detach_layernorm = True # Detaches the attention-block-output LayerNorm\n",
        "config.detach_mean = False # Detaches the attention-block-output LayerNorm\n",
        "config.detach_kq = True\n",
        "model = BertAttention(config, pretrained_embeds)\n",
        "model.load_state_dict(params)\n",
        "model.to(device)\n",
        "models['detach_KQ_LNorm_Norm'] = model"
      ],
      "metadata": {
        "id": "CcoBHCg-Qp_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Flipping\n",
        "UNK_token = tokenizer.unk_token_id\n",
        "\n",
        "fracs = np.linspace(0.,1.,11)"
      ],
      "metadata": {
        "id": "piQmmtURQrs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment cases if they are not computed already in all_flips\n",
        "gammas = [0.00,0.00, 0.00]\n",
        "for flip_case in [ 'generate' , 'pruning']:\n",
        "    print(flip_case)\n",
        "    all_flips = {}\n",
        "    \n",
        "    res_conservation = {}\n",
        "    for case, random_order in [('random', True), \n",
        "                               ('attn_last', False),  \n",
        "                               ('rollout_2', False),\n",
        "                               ('GAE' , False),\n",
        "                               ('gi', False),\n",
        "                               ('lrp_detach_KQ', False),\n",
        "                               ('lrp_detach_KQ_LNorm_Norm', False),\n",
        "                              ]:\n",
        "        print(case)\n",
        "        layer_idxs = model.attention_probs.keys()\n",
        "        M,E, EVOLUTION = [],[], []\n",
        "        C = []\n",
        "        j=0\n",
        "        if case in ['gi', 'lrp', 'GAE']:\n",
        "            model = models['none']\n",
        "        elif case in ['gi_detach_KQ', 'lrp_detach_KQ']:\n",
        "            model = models['detach_KQ']\n",
        "        elif case in ['detach_KQ_LNorm_Norm', 'lrp_detach_KQ_LNorm_Norm']   : \n",
        "            model = models['detach_KQ_LNorm_Norm']\n",
        "        else:\n",
        "            model = models['detach_KQ_LNorm']\n",
        "        for x in test_data_loader:\n",
        "            input_ids = torch.tensor(np.float32(x['input_ids']) , requires_grad=True).unsqueeze(0).long().to(device)\n",
        "            attention_mask = torch.tensor(np.float32(x['attention_mask']), requires_grad=True).unsqueeze(0).long().to(device)\n",
        "            token_type_ids = torch.tensor(np.float32(x['token_type_ids'])).unsqueeze(0).long().to(device)\n",
        "            words = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
        "            y_true = torch.tensor(x['label']).to(device)\n",
        "            labels_in = torch.tensor([int(y_true)]*len(input_ids)).long().to(device)\n",
        "            if case == 'GAE':\n",
        "                outs = model.forward_and_explain(input_ids=input_ids, cl=y_true,\n",
        "                 labels = labels_in , method=case)\n",
        "            else:\n",
        "                outs = model(input_ids=input_ids,\n",
        "                             labels = labels_in)\n",
        "            loss = outs['loss'].detach().cpu().numpy()\n",
        "            y_pred = np.argmax(outs['logits'].squeeze().detach().cpu().numpy())\n",
        "            if case =='random':\n",
        "                attribution = np.random.normal(0,1, np.array(words).shape) #np.random.normal(np.array(words).shape)\n",
        "\n",
        "            elif case == 'attn_last':\n",
        "                attribution = np.mean([x_.sum(0) for x_ in model.attention_probs[max(layer_idxs)].detach().cpu().numpy()[0]],0)\n",
        "            elif 'rollout' in case:\n",
        "                attns = [model.attention_probs[k].detach().cpu().numpy() for k in sorted(model.attention_probs.keys())]\n",
        "                attentions_mat = np.stack(attns,axis=0).squeeze()\n",
        "                res_att_mat = attentions_mat.sum(axis=1)/attentions_mat.shape[1]\n",
        "                joint_attentions = compute_joint_attention(res_att_mat, add_residual=True)\n",
        "                idx = int(case.replace('rollout_',''))\n",
        "                attribution = joint_attentions[idx].sum(0)\n",
        "            elif 'GAE' in case:\n",
        "                attns = [model.attention_probs[k].detach().cpu().numpy() for k in sorted(model.attention_probs.keys())]\n",
        "                attentions_mat = np.stack(attns,axis=0).squeeze()\n",
        "                attns = [model.attention_gradients[k].detach().cpu().numpy() for k in sorted(model.attention_gradients.keys())]\n",
        "                attentions_grads = np.stack(attns,axis=0).squeeze()\n",
        "                attentions_mat = torch.tensor(attentions_mat * attentions_grads).clamp(min=0)\n",
        "                attentions_mat = torch.tensor(attentions_mat).clamp(min=0).mean(dim=1)\n",
        "                joint_attentions = _compute_rollout_attention(attentions_mat)\n",
        "                joint_attentions[:, 0, 0] = 0\n",
        "                idx = 0\n",
        "                attribution = joint_attentions[idx].sum(0)\n",
        "            elif 'attention_flow' in case:\n",
        "                attns = [model.attention_probs[k].detach().cpu().numpy() for k in sorted(model.attention_probs.keys())]\n",
        "                attentions_mat = np.stack(attns,axis=0).squeeze()\n",
        "                idx = int(case.replace('attention_flow_',''))\n",
        "\n",
        "                aflow = get_flow_relevance_for_all_layers(input_ids.detach().cpu().numpy().squeeze(),  \n",
        "                                                          attentions_mat[:, np.newaxis],\n",
        "                                                          tokens=words, \n",
        "                                                          layers=[idx],\n",
        "                                                          pad_token= tokenizer.pad_token_id)\n",
        "                assert len(aflow) == 1\n",
        "                attribution =  aflow[0]\n",
        "            elif case == 'naive_gi':\n",
        "                outs = model.forward_and_explain(input_ids=input_ids, cl=y_true,\n",
        "                               labels = labels_in,\n",
        "                                 gammas = [0.0,0.0,0.0])\n",
        "\n",
        "                attribution = outs['R'].squeeze()\n",
        "\n",
        "\n",
        "            elif case in ['gi', 'gi_detach_KQ_LNorm', 'gi_detach_KQ', 'gi_detach_KQ_LNorm_Norm']:\n",
        "                outs = model.forward_and_explain(input_ids=input_ids, cl=y_true,\n",
        "                               labels = labels_in,\n",
        "                               gammas = [0.0,0.0,0.0])\n",
        "\n",
        "                attribution = outs['R'].squeeze()\n",
        "\n",
        "\n",
        "            elif case in ['lrp', 'lrp_detach_KQ_LNorm', 'lrp_detach_KQ', 'lrp_detach_KQ_LNorm_Norm']:\n",
        "\n",
        "                outs = model.forward_and_explain(input_ids=input_ids, cl=y_true,\n",
        "                               labels = labels_in, \n",
        "                                gammas = gammas)\n",
        "\n",
        "\n",
        "                attribution = outs['R'].squeeze()\n",
        "\n",
        "\n",
        "            if j==0:\n",
        "                print(type(attribution), type(input_ids),  type(words))\n",
        "\n",
        "            m, e, evolution = flip(model,\n",
        "                              x=attribution, \n",
        "                             token_ids=input_ids, \n",
        "                             tokens=words,\n",
        "                             y_true=y_true, \n",
        "                             fracs=fracs, \n",
        "                             flip_case=flip_case,\n",
        "                             random_order = random_order, \n",
        "                             tokenizer=tokenizer,\n",
        "                             device=device)\n",
        "\n",
        "\n",
        "            M.append(m)\n",
        "            E.append(e)\n",
        "            EVOLUTION.append(evolution)\n",
        "            \n",
        "\n",
        "            if j%500==0:\n",
        "                print('****',j)\n",
        "\n",
        "\n",
        "            j+=1\n",
        "        all_flips[case]= {'E':E, 'M':M, 'Evolution':EVOLUTION} \n",
        "\n",
        "        res_conservation[case] = C\n",
        "\n",
        "    f, axs = plt.subplots(1, 2, figsize=(14, 8))\n",
        "    for k, v in all_flips.items():\n",
        "        print(len(v['M']))\n",
        "        axs[0].plot(np.nanmean(v['M'], axis=0), label=k)\n",
        "        axs[0].set_title('($y_0$-$y_p$)$^2$')\n",
        "        axs[1].plot(np.nanmean(v['E'], axis=0), label=k)\n",
        "        axs[1].set_title('logits$_k$')    \n",
        "    plt.legend()\n",
        "    \n",
        "    f.savefig('{}.png'.format(flip_case), dpi=300)\n",
        "    pickle.dump(all_flips, open('all_flips_{}_sst.p'.format(flip_case), 'wb'))\n"
      ],
      "metadata": {
        "id": "MkrDSwpXQuqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}